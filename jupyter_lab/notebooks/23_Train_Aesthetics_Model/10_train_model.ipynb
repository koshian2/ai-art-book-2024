{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cd092c0-78fb-4bff-9aed-d86b06eb1a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from safetensors.torch import save_file\n",
    "import os\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a337e45-2f6f-4e97-b76f-2a97f3b35a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class to handle CSV and image data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, img_dir=\"AGIQA-3K\"):\n",
    "        self.data = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = f\"{self.img_dir}/{row['name']}\"\n",
    "        text = row['prompt']\n",
    "        for column in [\"adj1\", \"adj2\", \"style\"]:\n",
    "            if row[column] and not pd.isna(row[column]) and len(row[column].strip()) > 0:\n",
    "                text += f\", {row[column]}\"\n",
    "        label = (row[['mos_quality', 'mos_align']] / 5).tolist()\n",
    "\n",
    "        # print(text, type(text))\n",
    "        # print(label, type(label))\n",
    "        # print(image_path, type(image_path))\n",
    "        with Image.open(image_path) as image:\n",
    "            image = image.convert(\"RGB\")\n",
    "            inputs = self.processor(\n",
    "                text=[text], \n",
    "                images=image, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=\"max_length\", \n",
    "                max_length=self.processor.tokenizer.model_max_length, \n",
    "                truncation=True\n",
    "            )\n",
    "        inputs['label'] = torch.tensor(label, dtype=torch.float)\n",
    "        # print(inputs)\n",
    "         \n",
    "        return inputs\n",
    "\n",
    "# LightningModule class for the training loop\n",
    "class CLIPRegressionModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.clip_model.eval()  # Freeze CLIP model weights\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.text_projection = nn.Linear(self.clip_model.config.projection_dim, 512)\n",
    "        self.image_projection = nn.Linear(self.clip_model.config.projection_dim, 512)\n",
    "        self.norm_text_layer = nn.LayerNorm(512)\n",
    "        self.norm_image_layer = nn.LayerNorm(512)\n",
    "        self.final_layer = nn.Linear(512, 2)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def forward(self, text_inputs, image_inputs):\n",
    "        with torch.no_grad():\n",
    "            text_features = self.clip_model.get_text_features(**text_inputs)\n",
    "            image_features = self.clip_model.get_image_features(**image_inputs)\n",
    "        \n",
    "        text_proj = self.text_projection(text_features)\n",
    "        image_proj = self.image_projection(image_features)\n",
    "        dot_product = self.norm_text_layer(text_proj) * self.norm_image_layer(image_proj)\n",
    "        output = F.sigmoid(self.final_layer(dot_product))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text_inputs = {key: val.squeeze(1) for key, val in batch.items() if 'input_ids' in key}\n",
    "        image_inputs = {key: val.squeeze(1) for key, val in batch.items() if 'pixel_values' in key}\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = self(text_inputs, image_inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text_inputs = {key: val.squeeze(1) for key, val in batch.items() if 'input_ids' in key}\n",
    "        image_inputs = {key: val.squeeze(1) for key, val in batch.items() if 'pixel_values' in key}\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = self(text_inputs, image_inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "# Main training script\n",
    "def train_model(csv_file, img_dir, batch_size=128, max_epochs=30):\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    dataframe = pd.read_csv(csv_file)    \n",
    "    train_df, test_df = train_test_split(dataframe, test_size=0.1, random_state=42)\n",
    "    \n",
    "    train_dataset = CustomDataset(train_df, processor, img_dir=img_dir)\n",
    "    test_dataset = CustomDataset(test_df, processor, img_dir=img_dir)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_top_k=3,\n",
    "        filename='{epoch:02d}-{val_loss:.3f}',\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    model = CLIPRegressionModel()\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, devices=[0], accelerator=\"gpu\", callbacks=[checkpoint_callback])\n",
    "    trainer.fit(model, train_dataloader, test_dataloader)\n",
    "    return checkpoint_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39c75d0-a2c5-4f2c-b894-993f2afdacde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1]\n",
      "\n",
      "  | Name             | Type      | Params\n",
      "-----------------------------------------------\n",
      "0 | clip_model       | CLIPModel | 427 M \n",
      "1 | text_projection  | Linear    | 393 K \n",
      "2 | image_projection | Linear    | 393 K \n",
      "3 | norm_text_layer  | LayerNorm | 1.0 K \n",
      "4 | norm_image_layer | LayerNorm | 1.0 K \n",
      "5 | final_layer      | Linear    | 1.0 K \n",
      "6 | criterion        | L1Loss    | 0     \n",
      "-----------------------------------------------\n",
      "790 K     Trainable params\n",
      "427 M     Non-trainable params\n",
      "428 M     Total params\n",
      "1,713.628 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (21) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291479892e0d4cd1b365ff730d222a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 21: 'val_loss' reached 0.10066 (best 0.10066), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=00-val_loss=0.101.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 42: 'val_loss' reached 0.08894 (best 0.08894), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=01-val_loss=0.089.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 63: 'val_loss' reached 0.08378 (best 0.08378), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=02-val_loss=0.084.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 84: 'val_loss' reached 0.08245 (best 0.08245), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=03-val_loss=0.082.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 105: 'val_loss' reached 0.08157 (best 0.08157), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=04-val_loss=0.082.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 126: 'val_loss' reached 0.08075 (best 0.08075), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=05-val_loss=0.081.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 147: 'val_loss' reached 0.07913 (best 0.07913), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=06-val_loss=0.079.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 168: 'val_loss' reached 0.07881 (best 0.07881), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=07-val_loss=0.079.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 189: 'val_loss' reached 0.07722 (best 0.07722), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=08-val_loss=0.077.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 210: 'val_loss' reached 0.07645 (best 0.07645), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=09-val_loss=0.076.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 231: 'val_loss' reached 0.07671 (best 0.07645), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=10-val_loss=0.077.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 252: 'val_loss' reached 0.07637 (best 0.07637), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=11-val_loss=0.076.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 273: 'val_loss' reached 0.07528 (best 0.07528), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=12-val_loss=0.075.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 294: 'val_loss' reached 0.07595 (best 0.07528), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=13-val_loss=0.076.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 315: 'val_loss' reached 0.07477 (best 0.07477), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=14-val_loss=0.075.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 336: 'val_loss' reached 0.07513 (best 0.07477), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=15-val_loss=0.075.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 357: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 378: 'val_loss' reached 0.07436 (best 0.07436), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=17-val_loss=0.074.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 399: 'val_loss' reached 0.07474 (best 0.07436), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=18-val_loss=0.075.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 420: 'val_loss' reached 0.07399 (best 0.07399), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=19-val_loss=0.074.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 441: 'val_loss' reached 0.07396 (best 0.07396), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=20-val_loss=0.074.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 462: 'val_loss' reached 0.07382 (best 0.07382), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=21-val_loss=0.074.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 483: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 504: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 525: 'val_loss' reached 0.07356 (best 0.07356), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=24-val_loss=0.074.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 546: 'val_loss' reached 0.07318 (best 0.07318), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=25-val_loss=0.073.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 567: 'val_loss' reached 0.07303 (best 0.07303), saving model to '/home/23_Train_Aesthetics_Model/lightning_logs/version_4/checkpoints/epoch=26-val_loss=0.073.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 588: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 609: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 630: 'val_loss' was not in top 3\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "## Run train\n",
    "csv_file = \"AGIQA-3k-Database/data.csv\"\n",
    "img_dir = \"AGIQA-3K\"\n",
    "checkpoint_callback = train_model(csv_file, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee324d6-39fc-4efa-8222-b7a03b89d1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/23_Train_Aesthetics_Model/lightning_logs/version_3/checkpoints/epoch=29-val_loss=0.071.ckpt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d46222f-5ff6-4639-b90c-75c49c5e5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best checkpoint and save it as safetensors\n",
    "checkpoint = torch.load(checkpoint_callback.best_model_path, map_location=torch.device('cpu'))\n",
    "model_state_dict = checkpoint['state_dict']\n",
    "save_file(model_state_dict, \"agiqa_3k_clip_vitl14.safetensors\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
